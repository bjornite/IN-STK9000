{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN-STK5000 Credit Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02-10-2019\n",
    "#### Bj√∏rn Ivar Teigen, Mathieu Diaz, Jolynde Vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: Exploring and preprocessing the data\n",
    "We start by looking at the german credit data file. When we make a countplot of the outcome variable (which is whether a given loan was repaid, where 0 is no and 1 is yes), we see that the data is imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x106f02588>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "features = ['checking account balance', 'duration', 'credit history',\n",
    "            'purpose', 'amount', 'savings', 'employment', 'installment',\n",
    "            'marital status', 'other debtors', 'residence time',\n",
    "            'property', 'age', 'other installments', 'housing', 'credits',\n",
    "            'job', 'persons', 'phone', 'foreign']\n",
    "target = 'repaid'\n",
    "df = pd.read_csv('./data/german.data', sep=' ',\n",
    "                     names=features+[target])\n",
    "\n",
    "df.loc[df['repaid'] == 2, 'repaid'] = 0\n",
    "sns.countplot(x = df[target], data = df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['duration', 'age', 'residence time', 'installment', 'amount', 'duration', 'persons', 'credits']\n",
    "quantitative_features = list(filter(lambda x: x not in numerical_features, features))\n",
    "X = pd.get_dummies(df, columns=quantitative_features, drop_first=True)\n",
    "encoded_features = list(filter(lambda x: x != target, X.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of the outcome being 0, or not repaid, is 30%, while the percentage for the outcome being 1, or repaid, is 70%.\n",
    "\n",
    "Using one-hot encoding the categorical attributes are converted in order to be able to use them in the machine learning algorithm. Normalising is done in the models, and therefore not in the preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second: Policy design\n",
    "Designing a policy for giving or denying credit to individuals The choice for giving or denying credit to individuals is based on their probability for being credit-worthy. Given this probability, and taking into account the length of the loan, we can calculate the expected utility of giving a loan, using the formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E(U) = gain * p-amount*(1-p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where amount is the loaned amount, and gain is the total amount of interest on the loan. p is the predicted probability of the loan being paid back. The interest is calculated using the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "amount*((1+interest_rate)**(duration)/-1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where duration is loan duration in months, and interest_rate is return per month in %/100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gain(self, X):\n",
    "    return X['amount']*((1 + self.interest_rate)**(X['duration']) - 1)\n",
    "\n",
    "def expected_utility(self, X):\n",
    "    p = self.predict_proba(self.parse_X(X))\n",
    "    gain = self.calculate_gain(X)\n",
    "    expected_utilitiy = (gain.values*p.flatten()\n",
    "                        -X['amount'].values*(1-p.flatten()))\n",
    "    return expected_utilitiy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability is calculated by use of classification models.\n",
    "\n",
    "We assume that the labels represent the actual outcome of each loan, i.e. either loans are fully paid back or defaulted. We also assume the labeling process is accurate, i.e there is no noise in the labeling process.\n",
    "\n",
    "We've chosen to implement three different models so we can compare them. The models are kNN, random forest and a fully connected neural network.\n",
    "\n",
    "#### kNN:\n",
    "A kNN classifier with k=15 is used, pipelined with a standardscaler which subtracts the mean and scales input features to unit variance. The fit() function learns the means and standard deviations of each feature for the standardscaler, and then fits the kNN function to the training set. predict_proba() uses the in-build function in kNN from scikit learn.\n",
    "\n",
    "#### Random forest:\n",
    "Random forest does not scale the data. We use n=130 classifiers. predict_proba() uses the in-build function in random forest from scikit learn.\n",
    "\n",
    "#### Neural network:\n",
    "fit(): As a first layer for the model we use batch normalization. This centers and normalizes the input values. The main model is a simple fully connected artificial neural network with elu activations. We use L2 regularization. The final layer consists of a single neuron with a sigmoid activation. The network is trained using binary cross-entropy loss.\n",
    "\n",
    "We do a cross-validation (on the training data only) grid search over these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    param_grid = {'layer_sizes': [[32, 16], [64, 16], [64,32,16,8]],\n",
    "    'batch_size': [8],\n",
    "    'epochs': [3],\n",
    "    'interest_rate': [self.interest_rate],\n",
    "    'optimizer': ['Adam'],\n",
    "    'loss': ['binary_crossentropy'],\n",
    "    'alpha': [1, 0.1, 0.01, 0.001, 0.0001]}\n",
    "    self.model = GridSearchCV(NeuralBanker(), param_grid, cv=5, n_jobs=6)\n",
    "    self.model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scoring function for the grid search is the utility on the holdout set of the cross-validation.\n",
    "\n",
    "The selected model is trained using a batch size of 8 and 3 epochs of training with the Adam optimizer. layer_sizes is [64, 16], and the l2 regularization alpha parameter is 0.01.\n",
    "\n",
    "predict_proba(): This function merely outputs the result of running the trained network forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility\n",
    "Now we have models that are well trained and working. So we will be able to combine it with our policy for giving credit. We will retrieve the result of the function expected_utility(X). If the result is greater than 0, that is to say if we can make money with this loan, the action will take the value 1. If the value returned is 0 or negative, the loan must not be granted. Since we made a change of value at the beginning, we change the 0 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(self, X):\n",
    "    actions = (self.expected_utility(X) > 0).astype(int).flatten()\n",
    "    actions[np.where(actions == 0)] = 2\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of the policy\n",
    "Running all models through the TestLending procedure produced the following results:\n",
    "\n",
    "The table below shows an average utility per banker, over 5 runs with 0.5% and 5.0%, respectively, interest rate each month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Banker (class) | Amount <br/> (interest rate 0.5% / month) | Amount <br/> (interest rate 5% / month) |\n",
    "|:------|:------:|:------:|\n",
    "| Random Banker | -79560 | 841195 |\n",
    "| kNN Banker  | 1591 | 1256564 |\n",
    "| Random Forest Banker| 8837 | 1034688 |\n",
    "| Neural Network Banker | 4816 | 1102298 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Maximum* revenue can never be ensured, because there will always be an error rate in every machine learning model, and therefore there will always be some loss. However, by choosing a model with a low error on the data and by minimizing this error the revenue can be increased. The error can be minimized by optimizing the parameters of the model. \n",
    "\n",
    "To ensure an increase in revenue we compare the three different models (kNN, Random Forest and Neural Network) not only based on their accuracy score, but also on where their wrong predictions are located, seeing that some types of errors are more costly. In the case of credit loans, the *false positives* are a problem for the bank. False positives are the errors of the model where it falsely predicts a positive outcome, in our case that the loan will be repaid. This error is the most important to minimize, since this error costs the bank money. If the model falsely predicts a negative outcome, i.e. it predicts that the loan will not be repaid and therefore the credit loan will not be provided, it doesn‚Äôt cost the bank money. The confusion matrix gives the distribution of the predictions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | Accuracy score | Confusion matrix |\n",
    "|:------|:------:|:------:|\n",
    "| kNN Banker | 0.62 | [19  **42**] <br/> [15 124] |\n",
    "| Random Forest Banker | 0.645 | [31 **29**] <br/> [11 129] |\n",
    "| Neural Network Banker | ? | ? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right-upper corner of the matrix (in bold) shows the false positives of the model. We see that the random forest predicts less false positives and has a higher accuracy score. We will continue with the random forest and optimize this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Model optimization\n",
    "The ROC (Receiver Operating Characteristics) curve of the model shows true positive rate (x-axis) and the false positive rate (y-axis) dependent on the threshold used (the curve). A curve to the top and left is a better model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![alt text][logo]\n",
    "\n",
    "[logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In figure X we can see that the curve for the Random Forest model is directed to the top and left of the axis, showing a good model. \n",
    "\n",
    "However, since we are mostly interested in lowering the error of false positives of the model, whereas the false negatives are less important. The confusion matrix allows us to compute two metrics that deal with the false positives: \n",
    "* The specificity of the model, i.e. when the actual value is negative, how often is the prediction correct\n",
    "* The precision of the model, i.e. when a positive value is predicted, how often is the prediction correct \n",
    "\n",
    "Table X shows the scores for the Random Forest on these metrics for different probability thresholds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|| Specificity | Precision |\n",
    "|:------|:------:|:------:|\n",
    "| Threshold $p>0.5$ | 0.381 | 0.761 |\n",
    "| Threshold $p>0.7$ | 0.794 | 0.859 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the downside to increasing the probability threshold is that the model predicts more false negatives, meaning less people get a loan. The next step will be to see how it affects the actual utility or whether we should use the utilities instead of the probabilities. We have not yet figured this out however‚Ä¶ \n",
    "\n",
    "Increasing the threshold of the probability lowers the risk of the model being wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Model risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The risk of the model being wrong can be lowered by critically assessing the model‚Äôs performance. Beforehand, performance requirements for the model should be set in terms of accuracy score and acceptable error. These requirements are dependent on the data and the business model.\n",
    "\n",
    "After setting these initial requirements, they should be tested. A common measure is to calculate the accuracy score and the confusion matrix of the model, which are discussed before in model evaluation. By optimizing the parameters of the model these measures can be optimized. The confusion matrix can be improved by adjusting the threshold of the probability. The accuracy score can (in this case) be improved with feature selection and tuning the parameters of the model. By using the best subset of attributes that explain the relationship of the independent variables and the outcome variable, there is less noise from independent variables that do not explain the outcome variable that well. We will look at feature selection further in the next section. The optimal parameters of the model can be found by plotting them against the accuracy score of the model. \n",
    "\n",
    "By using cross validation, where the data is divided in k parts and each part is used as training data and once as test data for the model, more generalized relationships between the input and outcome data are achieved. The lower the standard deviation of the error scores from the cross-validation is, the better the model performs, because it does not vary a lot with different subsets of training data. This will ensure that the model performs better on unseen data as well. \n",
    "Lastly, it is important to keep monitoring the model to make sure it‚Äôs working well, and regularly retrain the model when new data comes in, so that the model keeps up to date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Limited or biased data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation, as explained before, is also a good way to deal with limited data. Rather than splitting the data into a training and a test part, where we would end up with a small test subset to test the model on, the whole dataset is used to both train and test over different runs, therefore making optimal use of the whole dataset. \n",
    "\n",
    "Uncertainty arising from biased data is hard to take into account. By critically assessing the origin of the data and how the data is collected biases in the dataset can be estimated. If these biases are clear up front, the design of the model can be adapted so that it responds minimal to these biases. \n",
    "\n",
    "After making the model it should be checked whether it is biased against certain societal biases. Marr (Forbes, 2019) lists steps that could be taken to minimize the risk of preserving societal biases in AI. Among others, the article focuses on ensuring that the algorithm is coded so that it doesn‚Äôt respond to societal biases. More specifically, this means that when designing a machine learning algorithm, it is important, first, to choose your subset carefully. In other words, make sure the subset is representative for the population you are predicting something for. Since we don‚Äôt collect our own data for this report this point is a bit difficult, but when using feature selection it is important to consider the population we are predicting. Second, in feature selection, we have to make sure we only exclude features that don‚Äôt influence the outcome. Because random forests uses decisions trees, where every tree builds different subsets (of the data) until it understands and represents the relationship of the variables with the target variable, this model has a feature importance attribute in it. When we plot this attribute, it gives the following graph for our random forest model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that duration, amount and age are the three most important features. However, due to the one-hot encoding, the graph is a bit hard to interpret. \n",
    "\n",
    "Another way to reduce a bias in the model is to monitor the performance of the model, thereby preventing that the model responds to societal biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
